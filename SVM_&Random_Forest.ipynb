{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24p7ptQxQrlh"
      },
      "source": [
        "# 1 - setup file locations for pre proccessed images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Q8hv8FgIHeB_",
        "outputId": "a912450d-61b5-4e5d-cfc1-8c1cd099fb24"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia3FbarZIwFX"
      },
      "outputs": [],
      "source": [
        "pip install opencv-python scikit-image numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NOfpuRVHyU1"
      },
      "outputs": [],
      "source": [
        "real  = \"/content/drive/MyDrive/ml_data_/Rprocessed\"\n",
        "fake = \"/content/drive/MyDrive/ml_data_/Fproccessed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIDvrZNJeR7u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.path.exists(real))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFss2pN5d9nP"
      },
      "outputs": [],
      "source": [
        "print(real)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm4QHEFKQ4A4"
      },
      "source": [
        "# manual feature extraction\n",
        "### methodes\n",
        "- Color Histograms\n",
        "- Texture Descriptors\n",
        "- Edge Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obNSDNteRNCS"
      },
      "outputs": [],
      "source": [
        "#1. Color Histograms (RGB or HSV Distributions)\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
        "    # Convert to HSV color space\n",
        "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    # Compute the histogram and normalize it\n",
        "    hist = cv2.calcHist([hsv_image], [0, 1, 2], None, bins, [0, 180, 0, 256, 0, 256])\n",
        "    hist = cv2.normalize(hist, hist).flatten()\n",
        "    return hist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZJGsh_CRizQ"
      },
      "outputs": [],
      "source": [
        "#2. Texture Descriptors (Gabor Filters)\n",
        "\n",
        "from skimage.filters import gabor\n",
        "\n",
        "def extract_gabor_features(image, frequency=0.6):\n",
        "    # Convert to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Apply Gabor filter\n",
        "    filtered, _ = gabor(gray_image, frequency=frequency)\n",
        "    return filtered.flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ja5bNciRrTf"
      },
      "outputs": [],
      "source": [
        "# 3. Edges (Sobel or Canny Edge Detection)\n",
        "def extract_edges(image, method='canny'):\n",
        "    # Convert to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    if method == 'canny':\n",
        "        # Canny edge detection\n",
        "        edges = cv2.Canny(gray_image, 100, 200)\n",
        "    elif method == 'sobel':\n",
        "        # Sobel edge detection\n",
        "        sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n",
        "        sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n",
        "        edges = np.sqrt(sobel_x**2 + sobel_y**2)\n",
        "\n",
        "    return edges.flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3X-HbHFR0gh"
      },
      "outputs": [],
      "source": [
        "def extract_combined_features(image):\n",
        "    color_features = extract_color_histogram(image)\n",
        "    texture_features = extract_gabor_features(image)\n",
        "    edge_features = extract_edges(image)\n",
        "    # Combine all features\n",
        "    combined_features = np.hstack((color_features, texture_features, edge_features))\n",
        "    return combined_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ8rBBEuciRH"
      },
      "source": [
        "# reading data set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVnLWRB4bGCO"
      },
      "outputs": [],
      "source": [
        "def process_dataset(real_dir, fake_dir):\n",
        "    X, y = [], []\n",
        "\n",
        "    # Process 'real' images\n",
        "    for img_name in os.listdir(real_dir):\n",
        "        img_path = os.path.join(real_dir, img_name)\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:  # Check if image is loaded correctly\n",
        "            print(f\"Error loading image {img_name}\")\n",
        "            continue\n",
        "        image = cv2.resize(image, (128, 128))  # resizing if it was not resized 3a4an el sizing issues\n",
        "\n",
        "        features = extract_combined_features(image) # the combined function that was made\n",
        "        features = features.flatten()  # Flatten the feature vector to 1D\n",
        "\n",
        "        X.append(features)\n",
        "        y.append(0)  # Label '0' for real images\n",
        "\n",
        "    # Process 'fake' images\n",
        "    for img_name in os.listdir(fake_dir):\n",
        "        img_path = os.path.join(fake_dir, img_name)\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:  # Check if image is loaded correctly\n",
        "            print(f\"Error loading image {img_name}\")\n",
        "            continue\n",
        "        image = cv2.resize(image,  (128, 128))  # Resize image\n",
        "\n",
        "        features = extract_combined_features(image)\n",
        "        features = features.flatten()  # Flatten the feature vector to 1D\n",
        "\n",
        "        X.append(features)\n",
        "        y.append(1)  # Label '1' for fake images\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "# Test the function\n",
        "X, y = process_dataset(real, fake)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3QgZX1OwThJ"
      },
      "source": [
        "# training basic **svm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XzYkO5dUzR1"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szgsp_V_Ip44"
      },
      "source": [
        "# standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHRc_QgujWoQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)  # Fit on training data and transform it\n",
        "X_test_scaled = scaler.transform(X_test)       # Transform the test data (use the same scaler)\n",
        "\n",
        "print(\"Standardization completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcYpKHEmj3YL"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# from sklearn.svm import SVC\n",
        "\n",
        "# # Define the model\n",
        "# svm = SVC()\n",
        "\n",
        "# # Define the hyperparameter grid\n",
        "# param_dist = {\n",
        "#     'C': [0.1, 1, 10, 100],\n",
        "#     'gamma': [1, 0.1, 0.01, 0.001],\n",
        "#     'kernel': ['rbf', 'linear']\n",
        "# }\n",
        "\n",
        "# # Perform Randomized Search\n",
        "# random_search = RandomizedSearchCV(svm, param_distributions=param_dist, n_iter=10, cv=3, scoring='accuracy', random_state=42)\n",
        "# random_search.fit(X_train, y_train)\n",
        "\n",
        "# # Print the best parameters and score\n",
        "# print(\"Best Parameters:\", random_search.best_params_)\n",
        "# print(\"Best Score:\", random_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7J7R2DES2RM"
      },
      "outputs": [],
      "source": [
        "# from sklearn.svm import SVC\n",
        "\n",
        "# # Define the SVM model with recommended hyperparameters\n",
        "# svm = SVC(C=10, gamma=0.001, kernel='rbf')\n",
        "\n",
        "# # Fit the model on training data\n",
        "# svm.fit(X_train, y_train)\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = svm.score(X_test, y_test)\n",
        "# print(f\"SVM Accuarcy : {accuracy * 100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.svm import SVC\n",
        "\n",
        "# # Define the SVM model with recommended hyperparameters\n",
        "# svm = SVC()\n",
        "\n",
        "# # Fit the model on training data\n",
        "# svm.fit(X_train, y_train)\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = svm.score(X_test, y_test)\n",
        "# print(f\"SVM Accuarcy : {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "Gm9vMblCEIyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjIAAz7-S4je"
      },
      "source": [
        "# Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBqZXWcLS92b"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # Example Data (replace X_train, X_test, y_train, y_test with your dataset)\n",
        "# # X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "# # Test different values for n_estimators\n",
        "# n_estimators_range = [10, 50, 100, 200, 300]\n",
        "# training_accuracies = []\n",
        "# validation_accuracies = []\n",
        "\n",
        "# # Loop through each value of n_estimators\n",
        "# for n in n_estimators_range:\n",
        "#     # Train the Random Forest model\n",
        "#     rf_model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "#     rf_model.fit(X_train, y_train)\n",
        "\n",
        "#     # Calculate training accuracy\n",
        "#     train_preds = rf_model.predict(X_train)\n",
        "#     train_accuracy = accuracy_score(y_train, train_preds)\n",
        "#     training_accuracies.append(train_accuracy)\n",
        "\n",
        "#     # Calculate validation accuracy\n",
        "#     val_preds = rf_model.predict(X_test)\n",
        "#     val_accuracy = accuracy_score(y_test, val_preds)\n",
        "#     validation_accuracies.append(val_accuracy)\n",
        "\n",
        "# # Plot the accuracy graph\n",
        "# plt.figure(figsize=[6, 4])\n",
        "# plt.plot(n_estimators_range, training_accuracies, 'black', linewidth=2.0, label='Training Accuracy')\n",
        "# plt.plot(n_estimators_range, validation_accuracies, 'blue', linewidth=2.0, label='Validation Accuracy')\n",
        "# plt.legend(fontsize=12)\n",
        "# plt.xlabel('Number of Estimators (n_estimators)', fontsize=10)\n",
        "# plt.ylabel('Accuracy', fontsize=10)\n",
        "# plt.title('Random Forest Accuracy Curves', fontsize=12)\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "MNHWz1betO3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST3eyi9OVTVW"
      },
      "outputs": [],
      "source": [
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # Initialize the Naive Bayes model\n",
        "# nb_model = GaussianNB()\n",
        "\n",
        "# # Train the Naive Bayes model on the standardized training data\n",
        "# nb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# # Make predictions on the test set\n",
        "# y_pred = nb_model.predict(X_test_scaled)\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(f\"Naive Bayes Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.svm import SVC\n",
        "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# # Split the data (assuming X and y are your features and labels)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Standardize the features\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training data\n",
        "# X_test_scaled = scaler.transform(X_test)  # Transform test data\n",
        "\n",
        "# # Initialize the base models\n",
        "# svm_model = SVC(probability=True, random_state=42)  # 'probability=True' for soft voting\n",
        "# rf_model = RandomForestClassifier(random_state=42)\n",
        "# nb_model = GaussianNB()\n",
        "\n",
        "# # Create a Voting Classifier\n",
        "# voting_classifier = VotingClassifier(\n",
        "#     estimators=[('svm', svm_model), ('rf', rf_model), ('nb', nb_model)],\n",
        "#     voting='soft'  # 'soft' for weighted probability voting, 'hard' for majority voting\n",
        "# )\n",
        "\n",
        "# # Train the ensemble model\n",
        "# voting_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# # Make predictions\n",
        "# y_pred = voting_classifier.predict(X_test_scaled)\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(f\"Ensemble Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "amx8Lnuh_KDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFSGFem8EAJ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}